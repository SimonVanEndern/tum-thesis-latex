% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Introduction}\label{chapter:introduction}
\section{Motivation}
%\section{Why we need an open-source location data approach}\label{chapter:introduction:section:motivation}

%\subsection{General motivation: We want to open-source data} \label{general-motivation}
“Data is the new oil” is a quote \parencite{data-is-the-new-oil, data-is-the-new-oil2} many people agree with. More and more businesses are based not on specific production capacities but on data and especially the ability to process it and the exclusive owenership over it. The success and monopoly of companies like Google, Facebook and Amazon can be attributed to this exclusive ownership at least to a reasonable extend.

While patents that used to power companies' success provide a balance through granting exclusive rights having to make the knowledge public, many companies e.g. Coca cola have decided successfully not to go for a patent and thus not reveal their knowledge \parencite{coca-cola}. If that approach is not compromised, it guarantees both - non-disclosure and also exclusive rights. Similarly, the non-disclosure of huge data sets collected by Facebook, Google and Amazon circumvent the balance intended by patents. Though, the unavailability of huge amounts of data to the public is an impediment of innovation and increased growth. For example, cities would benefit from aggregated location data in order to optimize traffic scheduling as also highlighted by \parencite{hoh2005protecting}.
Nevertheless, the publication of raw data sets is impossible because it severly intrudes the privacy of the owners of the data.

%\subsection{Main problem: Conflict between privacy and publishing user data}
So, even if companies would agree on a publication, a problem arises.
There is a conflict between preserving user privacy and publishing user data.

%\subsection{Existing privacy problem: The availability of central data sets}
Nevertheless, user privacy is already compromised even without publication of user data.
Already the mere existence of central data sets pose a privacy risk to users, because security issues might allow for theft and unwanted publication of these data.
An example is the theft of 14 million users' data from facebook \parencite{facebook}.

%\subsection{Relaxed privacy problems: The limited but not eliminated risk through a modified central data set}
Some governments and other institutions already publish some of their data sets after anonymizing them e.g. through cloaking of data so that it achieves k-anonymity and there are crowdsourcing and open source approaches to make data available to everybody. 
Nevertheless, the applied anonymization is often not sufficient or at least critical if the resulting data set should still be useful. Research shows that inferences can be drawn from the published data sets that violate the respecitve users' privacy \parencite{cellphone, twitter}. Also, most research is based on datasets that span only over a few days or weeks. When collecting location data over longer periods, the chance of successful inference attacks increases in all cases with more data available. So, in addition to the main risk of a central data set, publishing anonymized data poses another risk to users' privacy.

%\subsection{New problem in case of modified central data set: Trust to the server is needed}
Furthermore, besides the remaining risk of inference attacks on published anonymized data sets, the anonymization through those algorithms always depend on a trusted server to collect the data from all users and then publish the results of any analysis applying privacy-preserving algorithms beforehand. So even if the data is only stored anonymized on the server, besides the remaining risk of inference attacks, this still imposes a high privacy risk to every user, as trust can be misued by the trusted server itself.\\

\section{Research Questions}
In order to solve the problems, we pose the following research questions. (We assume trust to the server)
%: No central raw data set but only aggregated data}

\subsection*{RQ1: Is aggregation possible without raw data being accessible to anybody but the owner?}
% Or: What are the limitations that make this possible?
Previous work has shown that in order to overcome the conflicit between privacy intrusion and (public) data availability, a solution is needed that gets along without storing raw data in a central data set.
This solution should eliminate the risk of leaking raw user data through theft from a centralized database and also eliminate the remaining risk of inference attacks on published believed-to-be-anonymized raw data. We investigate whether there is a solution and what are the limitations, respectively which assumptions must hold for the solution to be valid.

\subsection*{RQ2: What types of aggregations (e.g. mean, median) can be published?}
We investigate how many details can be published without user privacy being compromised.

\subsection*{RQ3: What is the risk of inference attacks on aggregated data due to overlappings in the covered timespan?}
Previous work has mostly covered inference attacks on anonymized raw data sets. We will investigate, whether the same problems of inference attacks are possible on aggregated data as well and examine the limitations of publishing aggregated data.

\section{Contributions}
For our solution, we will focus on the sub-area of location data and location privacy.
We investigate the possibility of storing raw location data only decentralized on the collecting devices. On a central server available to the public, only aggregated data is stored, thus the main problem of privacy risk by a central database containing the overall raw data set is solved. We assume the central server to be trusted and also the clients to be trusted. As the aggregation process happens decentrally, the central server will never hold any other than aggregated data and never know about individual raw data. 

The structure of our research is organized as follows: First we review related work in the areas of location privacy and anonymisation techniques.
Chapter \ref{chapter:method} outlines our approach of decentralized data analysis and chapter \ref{chapter:design} documents the setup and explains design decisions. While we present the solution and the results of field-testing our setup in \ref{chapter:performance}, chapter \ref{chapter:conclusion} highlights limitations and future work. It also topics the limitations as of trust to the server and the clients and gives a short outline how this problem can be solved.

In summary, our approach takes the opposite direction as todays standard. We do not first collect the whole data set and then reduce it to a data set meeting privacy-constraints but we start from the bottom up - first by performing analysis in a decentralized manner so that there never is an overal data set imposing a security risk on all the entries' users, and second by proposing a framework that only releases aggregated data where no interference of any user information is possible. This data will then be available to the public. This gives us maximum possible feedback on eventual privacy problems, creates trust through transparency and fosters innovation through availability of data to everyone.

%and supports the process of not randomly collecting data and afterwards researching on metrics that are actually needed but first on evaluating which metrices are needed and then retrieving them if possible without raising privacy concerns.

%\section{Outline}
%The structure of our research is organized as follows: First we re-
%view related work in the areas of location privacy and anonymisation techniques. In
%section \ref{chapter:solution} we describe our approach of decentralized data analysis to get along without a central database.
%Section XXX describes the setup in detail. Section XXX analysis the result from field-testing our application.
%Section XXX incorporates the results into our proposal of a possibility to achieve 100\% privacy through all applications.
%Section XXX summarizes our work and points out further research possibilities.

%The rest of this research is organized as f

%But also privacy concerns of users have increased due to leakages where their data was not well protected at e.g. facebook and stolen and published.



%So, we identify two related issues compromising data privacy. 
%\begin{enumerate}
%  \item The availability of huge data sets at central servers imposes a risk stemming from the computer science area of security. (main risk)
%  \item Publication of entire data sets can even after applying anonymization techniques not guarantee privacy preservation. (reduced risk)
%\end{enumerate}

%\subsection{Examples of direct and indirect privacy breaches}


%An example for the second issue is that the location data of Twitter tweets was published without asking the user for permission. Furthermore this data is only available through the API, so that the user is not aware of this infringement. Using this data, ~\parencite{twitter} has shown that this data can be used to infer a users home address and often also the work address, even if the user itself is privacy-aware, thus does not publish his / her name, etc.



%\begin{itemize}
%  \item Collect less data \parencite{privacy-home-work-pairs}
%  \item Mixing approach \parencite{location-privacy}
%  \item Anonymize data to meet the kriteria of k-anonymity \parencite{k-anonymity} and \parencite{cellphone}
%  \item spatial cloaking \parencite{krumm}
%  \item Remove not only identifiers from the data set but also apply algorithms, that remove samples, that can be (due to few samples in this area) identified \parencite{time-to-confusion}
%\end{itemize}

%\subsection{Problems that still arise}

%Still this privacy is only limited if only this one data set is taken into account. If e.g. multiple of those data sets from different data collectors are combined, or information about an individual like home and work adress is provided, privacy breaches are still highly likely.



