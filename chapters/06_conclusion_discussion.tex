% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.
\chapter{Conclusion and Discussion}\label{chapter:conclusion}
We have shown that our hypothesis holds but only for aggregated data. This is fine because except in one experimental setting as with trajectories, there is no need for (anonymized) raw data. Also the experimental setting could be replaced by directly implementing the aggregations and testing with a greater user base. In theory, the anonymity and preserved privacy that hold for the tested aggregations hohlds also for the other proposed aggregations and aggregations we have not evaluated here. In order to follow with this research, we provide the setup to easily implement and test those and further aggregations in future research in order to further support our hypothesis.

\section{Limitations}
\begin{itemize}
	\item As mentioned in XX, our system is based on trust. In case of user data being compromised, this significantly impacts some of the results. Nevertheless, collecting the list of mean values is far less error prone as the outlier could also be identified.
\end{itemize}

\section{Future Work}
\begin{itemize}
	\item While XX has found that inference attacks can be based on the same dataset being published two times with different anonymization techniques applied and XX shows that anonymized datasets that overlap poses a risk, it still has to be investigated whether overlapping aggregated data as in our case can pose a risk.
	\item Some of the techniques identified as useful, such as spatial cloaking, ... should be applied in our setting.
	\item Our framework would also allow to pre-populate (simulated) smartphones with artificially generated or otherhow collected data in order to test and verify the functionality.
	\item Another use of our framework is the area of decentralized computation. Problems might be solved locally and collected by the server afterwards and the pieces put together still with anonymity for the users.
	\item Ask the user for his / her home and work location or infer it from the data in order to process aggregation requests as mentioned in section XX.
	\item Store the users location on the server (granularity level approach) in order to allow for aggregation requests targeted at specific areas and not the overall user base. (levels and level database and unlocked levels collection necessary)
	\item Android: Only send a final aggregation back to the server when criteria like minimum n, ... are met.
	\item Server: Apply that the request has a counter how many times it was actually retrieved, so that after e.g. 10 times it was retrieved but never answered, the request is rolled back because apparently the user somehow cannot process the request.
	\item Have a nice activity informing the user / displaying some information to the user.
	\item (Put somewhere else!! TODO) The app can send traffic alerts to the server if it is on a route where usually traffic is far faster. (This also via other nodes in order to not letting the server know who is on this route.)
	\item a scheduler which automatically creates aggregation requests on a regular basis so that not as now Postman has to be used to start requests. The Postman collections used during field testing can be found here : XXX
	\item Generate userList of aggregation request dynamically.
	\item Pre-populate (raw)aggregation requests so that the first users cannot infer data from the users before them with high probabilities. Upon receiving the final result, the server can look up the used initialization values from the rawAggregationRequest and calculate the actual correct result and insert it into the database.
	\item Generate some use for the user of the application e.g. through showing locally aggregated data and comparing it to aggregated data publicly available e.g. in order to show how many percent walked more this day than the respective user. This is also an approach to motivate app installations in case of a broader user base necessary.
	\item delete local data after some time.
	\item Evaluate how many people have to participate in an aggreation request to be representative / how to chose users participating in it in order to not be biased (e.g. when taking always the most recent active users, the users only online a few times are day are discriminated against)
	\item Investigate overlapping
	\item When returning list data to the server, the list order should be randomised by the user before sending.
	\item Verify public key e.g. through telephone number passing and verifying this way and then building a network of verifications.
	\item Adhere to standards of schema.org
	\item In the future it should be implemented that devices change the public / private key pair from time to time
	\item It could also be implemented to verify another public key but requesting an SMS, .... (harder with changing public-private key-pairs)
	\item Possible future aggregation: List of all activity times e.g. for walking to find out mean, medium, ...
	\item When dynamically adding users to the aggregation request, we need another field for setting the limit so that when the limit is reached, the user sends back the result.
	\item Pre-fill the aggregation request so that the second user cannot read the first user's share.
\end{itemize}
We suggest to do especially two things in any further research project: Implement error logging and sending those errors to the server in order to find and remove bugs. Secondly bring the application to the playstore so that updates are possible (even without user interaction in case of automatic updates) and be able to activate a broader user base while still working on the final version to be tested.

Also, the approach could easily integrated into existing projects like open maps or be build modular in order to allow using it as a library with other applications.

\section{Evaluation}
The results support our hypothesis that data can be analyzed decentrally and that aggregated data can be published without any privacy concerns. The aggregations of mean values clearly leave no doubt about full privacy protection as there even is no personal data involved anymore. The listing of mean values of average number of steps per participant allows for more advanced statistical analysis while at the same time the values cannot be mapped to persons. Even when conducting the same request twice, due to users being chosen dynamically, one could most probably see if the same user participated in the second request but nothing else. When aggregating over another time period (that might have an intersection with the other one), there is not even the chance to identify whether the same user participated in both aggregations. As requests are started not at the same time (TODO!!!), and users are in a future setting allocated dynamically to the request, there is also no chance to link the data from different aggregations. From the number of steps one could infer the time somebody spent walking, but as it is not given whether the steps where conducting walking, running or both, this linking would result in a very poor performance and also only reveal that to a very low probability, the user with X steps in one aggregatioin is the same user spending X minutes walking the same day. 
The listing of trajectories created a dataset that clearly shows the vulnerability highlighted in XX and XX. Nevertheless, this is no aggregation but just a collection of raw data with stripped of identifiers and timestamps anonymized to a daily basis. The results show clearly that our setup is sufficiently accurate to field test the other aggregations proposed in XX and further prove our thesis. The data shows, that e.g. A change of transportation system can clearly be identified. 

\section{Limitations}
As stated in the introduction, our approach is based on trust among the clients and the server. Nevertheless, while e.g. \parencite{crowdsourcing} face the same problems in case of a compromised server - namely that the server can create artificial participants and thus obtain the raw values from each user, our setup is more general, allows for more complex aggregations and can be adapted to work via P2P or to establish trust through something like XXX